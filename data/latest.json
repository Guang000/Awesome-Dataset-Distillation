{
  "2025/12/22": [
    {
      "title": "DiRe: Diversity-promoting Regularization for Dataset Condensation",
      "author": "Saumyaranjan Mohanty et al., WACV 2026",
      "github": "https://github.com/DIL-IITH/DiRe ",
      "url": "https://arxiv.org/abs/2512.13083",
      "cite": "mohanty2026dire",
      "website": null
    }
  ],
  "2025/12/18": [
    {
      "title": "Distill Video Datasets into Images",
      "author": "Zhenghao Zhao et al., 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2512.14621",
      "cite": "zhao2025video",
      "website": null
    },
    {
      "title": "EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training",
      "author": "Yuting Tang et al., AAAI 2026",
      "github": "https://github.com/t170815518/EEG-DLite ",
      "url": "https://arxiv.org/abs/2512.12210",
      "cite": "tang2026eeg",
      "website": null
    }
  ],
  "2025/12/05": [
    {
      "title": "TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution",
      "author": "Fengli Ran et al., AAAI 2026",
      "github": "https://github.com/FlyFinley/TGDD ",
      "url": "https://arxiv.org/abs/2512.02469",
      "cite": "ran2026tgdd",
      "website": null
    }
  ],
  "2025/11/25": [
    {
      "title": "Decoupled Audio-Visual Dataset Distillation",
      "author": "Wenyuan Li & Guang Li et al., 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2511.17890",
      "cite": "li2025davdd",
      "website": null
    },
    {
      "title": "DDTime: Dataset Distillation with Spectral Alignment and Information Bottleneck for Time-Series Forecasting",
      "author": "Yuqi Li & Kuiye Ding et al., 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2511.16715",
      "cite": "li2025time",
      "website": null
    },
    {
      "title": "Low-Level Dataset Distillation for Medical Image Enhancemen",
      "author": "Fengzhi Xu et al., 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2511.13106",
      "cite": "xu2025low",
      "website": null
    },
    {
      "title": "Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillation",
      "author": "Shuhan Ye et al., 2025",
      "github": null,
      "url": "https://arxiv.org/abs/2511.12095",
      "cite": "ye2025snn",
      "website": null
    },
    {
      "title": "Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling",
      "author": "Xiao Cui et al., AAAI 2026",
      "github": null,
      "url": "https://arxiv.org/abs/2511.18858",
      "cite": "cui2026long",
      "website": null
    }
  ],
  "2025/11/20": [
    {
      "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models",
      "author": "George Cazenavette et al., NeurIPS 2025",
      "github": "https://github.com/GeorgeCazenavette/linear-gradient-matching ",
      "url": "https://arxiv.org/abs/2511.16674",
      "cite": "cazenavette2025dataset",
      "website": "https://linear-gradient-matching.github.io/ "
    }
  ]
}