{
  "main": {
    "base": [
      {
        "title": "Dataset Distillation",
        "author": "Tongzhou Wang et al., 2018",
        "github": "https://github.com/SsnL/dataset-distillation",
        "url": "https://arxiv.org/abs/1811.10959",
        "cite": "wang2018datasetdistillation",
        "website": "https://ssnl.github.io/dataset_distillation/"
      }
    ],
    "early": [
      {
        "title": "Gradient-Based Hyperparameter Optimization Through Reversible Learning",
        "author": "Dougal Maclaurin et al., ICML 2015",
        "github": "https://github.com/HIPS/hypergrad",
        "url": "https://arxiv.org/abs/1502.03492",
        "cite": "maclaurin2015gradient",
        "website": null
      }
    ],
    "gradient": [
      {
        "title": "Dataset Condensation with Gradient Matching",
        "author": "Bo Zhao et al., ICLR 2021",
        "github": "https://github.com/VICO-UoE/DatasetCondensation",
        "url": "https://arxiv.org/abs/2006.05929",
        "cite": "zhao2021datasetcondensation",
        "website": null
      },
      {
        "title": "Dataset Condensation with Differentiable Siamese Augmentation",
        "author": "Bo Zhao et al., ICML 2021",
        "github": "https://github.com/VICO-UoE/DatasetCondensation",
        "url": "https://arxiv.org/abs/2102.08259",
        "cite": "zhao2021differentiatble",
        "website": null
      },
      {
        "title": "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation",
        "url": "https://arxiv.org/abs/2507.05914",
        "author": "Sheng-Feng Yu et al., ICLR 2025",
        "github": null,
        "cite": "yu2025self",
        "website": null
      },
      {
        "title": "Dataset Distillation by Matching Training Trajectories",
        "author": "George Cazenavette et al., CVPR 2022",
        "github": "https://github.com/georgecazenavette/mtt-distillation",
        "url": "https://arxiv.org/abs/2203.11932",
        "cite": "cazenavette2022dataset",
        "website": "https://georgecazenavette.github.io/mtt-distillation/"
      },
      {
        "title": "Dataset Condensation with Contrastive Signals",
        "author": "Saehyung Lee et al., ICML 2022",
        "github": "https://github.com/saehyung-lee/dcc",
        "url": "https://arxiv.org/abs/2202.02916",
        "cite": "lee2022dataset",
        "website": null
      },
      {
        "title": "Loss-Curvature Matching for Dataset Selection and Condensation",
        "author": "Seungjae Shin & Heesun Bae et al., AISTATS 2023",
        "github": "https://github.com/SJShin-AI/LCMat",
        "url": "https://arxiv.org/abs/2303.04449",
        "cite": "shin2023lcmat",
        "website": null
      },
      {
        "title": "Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation",
        "author": "Jiawei Du & Yidi Jiang et al., CVPR 2023",
        "github": "https://github.com/AngusDujw/FTD-distillation",
        "url": "https://arxiv.org/abs/2211.11004",
        "cite": "du2023minimizing",
        "website": null
      },
      {
        "title": "Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory",
        "author": "Justin Cui et al., ICML 2023",
        "github": "https://github.com/justincui03/tesla",
        "url": "https://arxiv.org/abs/2211.10586",
        "cite": "cui2022scaling",
        "website": null
      },
      {
        "title": "Sequential Subset Matching for Dataset Distillation",
        "author": "Jiawei Du et al., NeurIPS 2023",
        "github": "https://github.com/shqii1j/seqmatch",
        "url": "https://arxiv.org/abs/2311.01570",
        "cite": "du2023seqmatch",
        "website": null
      },
      {
        "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching",
        "author": "Ziyao Guo & Kai Wang et al., ICLR 2024",
        "github": "https://github.com/GzyAftermath/DATM",
        "url": "https://arxiv.org/abs/2310.05773",
        "cite": "guo2024datm",
        "website": "https://gzyaftermath.github.io/DATM/"
      },
      {
        "title": "SelMatch: Effectively Scaling Up Dataset Distillation via Selection-Based Initialization and Partial Updates by Trajectory Matching",
        "author": "Yongmin Lee et al., ICML 2024",
        "github": "https://github.com/Yongalls/SelMatch",
        "url": "https://arxiv.org/abs/2406.18561",
        "cite": "lee2024selmatch",
        "website": null
      },
      {
        "title": "Dataset Distillation by Automatic Training Trajectories",
        "author": "Dai Liu et al., ECCV 2024",
        "github": "https://github.com/NiaLiu/ATT",
        "url": "https://arxiv.org/abs/2407.14245",
        "cite": "liu2024att",
        "website": null
      },
      {
        "title": "Neural Spectral Decomposition for Dataset Distillation",
        "author": "Shaolei Yang et al., ECCV 2024",
        "github": "https://github.com/slyang2021/NSD",
        "url": "https://arxiv.org/abs/2408.16236",
        "cite": "yang2024nsd",
        "website": null
      },
      {
        "title": "Prioritize Alignment in Dataset Distillation",
        "author": "Zekai Li & Ziyao Guo et al., 2024",
        "github": "https://github.com/NUS-HPC-AI-Lab/PAD",
        "url": "https://arxiv.org/abs/2408.03360",
        "cite": "li2024pad",
        "website": null
      },
      {
        "title": "Towards Stable and Storage-efficient Dataset Distillation: Matching Convexified Trajectory",
        "author": "Wenliang Zhong et al., CVPR 2025",
        "github": "https://github.com/Zhong0x29a/MCT",
        "url": "https://arxiv.org/abs/2406.19827",
        "cite": "zhong2025mct",
        "website": null
      },
      {
        "title": "Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios",
        "author": "Kai Wang & Zekai Li et al., CVPR 2025",
        "github": "https://github.com/NUS-HPC-AI-Lab/EDF",
        "url": "https://arxiv.org/abs/2410.17193",
        "cite": "wang2025edf",
        "website": null
      }
    ],
    "distribution": [
      {
        "title": "CAFE: Learning to Condense Dataset by Aligning Features",
        "author": "Kai Wang & Bo Zhao et al., CVPR 2022",
        "github": "https://github.com/kaiwang960112/cafe",
        "url": "https://arxiv.org/abs/2203.01531",
        "cite": "wang2022cafe",
        "website": null
      },
      {
        "title": "Dataset Condensation with Distribution Matching",
        "author": "Bo Zhao et al., WACV 2023",
        "github": "https://github.com/VICO-UoE/DatasetCondensation",
        "url": "https://arxiv.org/abs/2110.04181",
        "cite": "zhao2023distribution",
        "website": null
      },
      {
        "title": "Improved Distribution Matching for Dataset Condensation",
        "author": "Ganlong Zhao et al., CVPR 2023",
        "github": "https://github.com/uitrbn/IDM",
        "url": "https://arxiv.org/abs/2307.09742",
        "cite": "zhao2023idm",
        "website": null
      },
      {
        "title": "DataDAM: Efficient Dataset Distillation with Attention Matching",
        "author": "Ahmad Sajedi & Samir Khaki et al., ICCV 2023",
        "github": "https://github.com/DataDistillation/DataDAM",
        "url": "https://arxiv.org/abs/2310.00093",
        "cite": "sajedi2023datadam",
        "website": "https://datadistillation.github.io/DataDAM/"
      },
      {
        "title": "M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy",
        "author": "Hansong Zhang & Shikun Li et al., AAAI 2024",
        "github": "https://github.com/Hansong-Zhang/M3D",
        "url": "https://arxiv.org/abs/2312.15927",
        "cite": "zhang2024m3d",
        "website": null
      },
      {
        "title": "Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation",
        "author": "Wenxiao Deng et al., CVPR 2024",
        "github": "https://github.com/VincenDen/IID",
        "url": "https://arxiv.org/abs/2404.00563",
        "cite": "deng2024iid",
        "website": null
      },
      {
        "title": "Dataset Condensation with Latent Quantile Matching",
        "author": "Wei Wei et al., CVPR 2024 Workshop",
        "github": null,
        "url": "https://openaccess.thecvf.com/content/CVPR2024W/DDCV/html/Wei_Dataset_Condensation_with_Latent_Quantile_Matching_CVPRW_2024_paper.html",
        "cite": "wei2024lqm",
        "website": null
      },
      {
        "title": "DANCE: Dual-View Distribution Alignment for Dataset Condensation",
        "author": "Hansong Zhang et al., IJCAI 2024",
        "github": "https://github.com/Hansong-Zhang/DANCE",
        "url": "https://arxiv.org/abs/2406.01063",
        "cite": "zhang2024dance",
        "website": null
      },
      {
        "title": "Diversified Semantic Distribution Matching for Dataset Distillation",
        "author": "Hongcheng Li et al., MM 2024",
        "github": "https://github.com/Li-Hongcheng/DSDM",
        "url": "https://dl.acm.org/doi/10.1145/3664647.3680900",
        "cite": "li2024dsdm",
        "website": null
      },
      {
        "title": "Dataset Distillation with Neural Characteristic Function: A Minmax Perspective",
        "author": "Shaobo Wang et al., CVPR 2025",
        "github": "https://github.com/gszfwsb/NCFM",
        "url": "https://arxiv.org/abs/2502.20653",
        "cite": "wang2025ncfm",
        "website": null
      },
      {
        "title": "OPTICAL: Leveraging Optimal Transport for Contribution Allocation in Dataset Distillation",
        "author": "Xiao Cui et al., CVPR 2025",
        "github": null,
        "url": "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation_CVPR_2025_paper.html",
        "cite": "cui2025optical",
        "website": null
      },
      {
        "title": "Dataset Distillation with Feature Matching through the Wasserstein Metric",
        "author": "Haoyang Liu et al., ICCV 2025",
        "github": "https://github.com/Liu-Hy/WMDD",
        "url": "https://arxiv.org/abs/2311.18531",
        "cite": "liu2025wasserstein",
        "website": "https://liu-hy.github.io/WMDD/"
      },
      {
        "title": "Hyperbolic Dataset Distillation",
        "author": "Wenyuan Li & Guang Li et al., 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2505.24623",
        "cite": "li2025hdd",
        "website": null
      }
    ],
    "kernel": [
      {
        "title": "Dataset Meta-Learning from Kernel Ridge-Regression",
        "author": "Timothy Nguyen et al., ICLR 2021",
        "github": "https://github.com/google/neural-tangents",
        "url": "https://arxiv.org/abs/2011.00050",
        "cite": "nguyen2021kip",
        "website": null
      },
      {
        "title": "Dataset Distillation with Infinitely Wide Convolutional Networks",
        "author": "Timothy Nguyen et al., NeurIPS 2021",
        "github": "https://github.com/google/neural-tangents",
        "url": "https://arxiv.org/abs/2107.13034",
        "cite": "nguyen2021kipimprovedresults",
        "website": null
      },
      {
        "title": "Dataset Distillation using Neural Feature Regression",
        "author": "Yongchao Zhou et al., NeurIPS 2022",
        "github": "https://github.com/yongchao97/FRePo",
        "url": "https://arxiv.org/abs/2206.00719",
        "cite": "zhou2022dataset",
        "website": "https://sites.google.com/view/frepo"
      },
      {
        "title": "Efficient Dataset Distillation using Random Feature Approximation",
        "author": "Noel Loo et al., NeurIPS 2022",
        "github": "https://github.com/yolky/RFAD",
        "url": "https://arxiv.org/abs/2210.12067",
        "cite": "loo2022efficient",
        "website": null
      },
      {
        "title": "Dataset Distillation with Convexified Implicit Gradients",
        "author": "Noel Loo et al., ICML 2023",
        "github": "https://github.com/yolky/RCIG",
        "url": "https://arxiv.org/abs/2302.06755",
        "cite": "loo2023dataset",
        "website": null
      },
      {
        "title": "Provable and Efficient Dataset Distillation for Kernel Ridge Regression",
        "author": "Yilan Chen et al., NeurIPS 2024",
        "github": null,
        "url": "https://openreview.net/forum?id=WI2VpcBdnd",
        "cite": "chen2024krr",
        "website": null
      }
    ],
    "parametrization": [
      {
        "title": "Dataset Condensation via Efficient Synthetic-Data Parameterization",
        "author": "Jang-Hyun Kim et al., ICML 2022",
        "github": "https://github.com/snu-mllab/efficient-dataset-condensation",
        "url": "https://arxiv.org/abs/2205.14959",
        "cite": "kim2022dataset",
        "website": null
      },
      {
        "title": "Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks",
        "author": "Zhiwei Deng et al., NeurIPS 2022",
        "github": "https://github.com/princetonvisualai/RememberThePast-DatasetDistillation",
        "url": "https://arxiv.org/abs/2206.02916",
        "cite": "deng2022remember",
        "website": null
      },
      {
        "title": "On Divergence Measures for Bayesian Pseudocoresets",
        "author": "Balhae Kim et al., NeurIPS 2022",
        "github": "https://github.com/balhaekim/bpc-divergences",
        "url": "kim2022divergence",
        "cite": "https://arxiv.org/abs/2210.06205",
        "website": null
      },
      {
        "title": "Dataset Distillation via Factorization",
        "author": "Songhua Liu et al., NeurIPS 2022",
        "github": "https://github.com/Huage001/DatasetFactorization",
        "url": "https://arxiv.org/abs/2210.16774",
        "cite": "liu2022dataset",
        "website": null
      },
      {
        "title": "PRANC: Pseudo RAndom Networks for Compacting Deep Models",
        "author": "Parsa Nooralinejad et al., 2022",
        "github": "https://github.com/UCDvision/PRANC",
        "url": "https://arxiv.org/abs/2206.08464",
        "cite": "nooralinejad2022pranc",
        "website": null
      },
      {
        "title": "Dataset Condensation with Latent Space Knowledge Factorization and Sharing",
        "author": "Hae Beom Lee & Dong Bok Lee et al., 2022",
        "github": null,
        "url": "lee2022kfs",
        "cite": "https://arxiv.org/abs/2208.10494",
        "website": null
      },
      {
        "title": "Slimmable Dataset Condensation",
        "author": "Songhua Liu et al., CVPR 2023",
        "github": null,
        "url": "https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Slimmable_Dataset_Condensation_CVPR_2023_paper.html",
        "cite": "liu2023slimmable",
        "website": null
      },
      {
        "title": "Few-Shot Dataset Distillation via Translative Pre-Training",
        "author": "Songhua Liu et al., ICCV 2023",
        "github": null,
        "url": "https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Few-Shot_Dataset_Distillation_via_Translative_Pre-Training_ICCV_2023_paper.html",
        "cite": "liu2023fewshot",
        "website": null
      },
      {
        "title": "MGDD: A Meta Generator for Fast Dataset Distillation",
        "author": "Songhua Liu et al., NeurIPS 2023",
        "github": null,
        "url": "https://openreview.net/forum?id=D9CMRR5Lof",
        "cite": "liu2023mgdd",
        "website": null
      },
      {
        "title": "Sparse Parameterization for Epitomic Dataset Distillation",
        "author": "Xing Wei & Anjia Cao et al., NeurIPS 2023",
        "github": "https://github.com/MIV-XJTU/SPEED",
        "url": "https://openreview.net/forum?id=ZIfhYAE2xg",
        "cite": "wei2023sparse",
        "website": null
      },
      {
        "title": "Frequency Domain-based Dataset Distillation",
        "author": "Donghyeok Shin & Seungjae Shin et al., NeurIPS 2023",
        "github": "https://github.com/sdh0818/FreD",
        "url": "https://arxiv.org/abs/2311.08819",
        "cite": "shin2023fred",
        "website": null
      },
      {
        "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation",
        "author": "Haizhong Zheng et al., ECCV 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2310.07506",
        "cite": "zheng2024hmn",
        "website": null
      },
      {
        "title": "FYI: Flip Your Images for Dataset Distillation",
        "author": "Byunggwan Son et al., ECCV 2024",
        "github": "https://github.com/cvlab-yonsei/FYI",
        "url": "https://arxiv.org/abs/2407.08113",
        "cite": "son2024fyi",
        "website": "https://cvlab.yonsei.ac.kr/projects/FYI/"
      },
      {
        "title": "Color-Oriented Redundancy Reduction in Dataset Distillation",
        "author": "Bowen Yuan et al., NeurIPS 2024",
        "github": "https://github.com/KeViNYuAn0314/AutoPalette",
        "url": "https://arxiv.org/abs/2411.11329",
        "cite": "yuan2024color",
        "website": null
      },
      {
        "title": "Distilling Dataset into Neural Field",
        "author": "Donghyeok Shin et al., ICLR 2025",
        "github": "https://github.com/aailab-kaist/DDiF",
        "url": "https://arxiv.org/abs/2503.04835",
        "cite": "shin2025ddif",
        "website": null
      },
      {
        "title": "Dataset Distillation as Data Compression: A Rate-Utility Perspective",
        "author": "Youneng Bao & Yiping Liu et al., ICCV 2025",
        "github": "https://github.com/nouise/DD-RUO",
        "url": "https://arxiv.org/abs/2507.17221",
        "cite": "bao2025ruo",
        "website": "https://nouise.github.io/DD-RUO/"
      }
    ],
    "generative": [

    ],
    "_gan": [
      {
        "title": "Synthesizing Informative Training Samples with GAN",
        "author": "https://github.com/vico-uoe/it-gan",
        "github": "Bo Zhao et al., NeurIPS 2022 Workshop",
        "url": "https://arxiv.org/abs/2204.07513",
        "cite": "zhao2022synthesizing",
        "website": null
      },
      {
        "title": "Generalizing Dataset Distillation via Deep Generative Prior",
        "author": "George Cazenavette et al., CVPR 2023",
        "github": "https://github.com/georgecazenavette/glad",
        "url": "https://arxiv.org/abs/2305.01649",
        "cite": "cazenavette2023glad",
        "website": "https://georgecazenavette.github.io/glad/"
      },
      {
        "title": "DiM: Distilling Dataset into Generative Model",
        "author": "Kai Wang & Jianyang Gu et al., 2023",
        "github": "https://github.com/vimar-gu/DiM",
        "url": "https://arxiv.org/abs/2303.04707",
        "cite": "wang2023dim",
        "website": null
      },
      {
        "title": "Dataset Condensation via Generative Model",
        "author": "Junhao Zhang et al., 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2309.07698",
        "cite": "zhang2023dc",
        "website": null
      },
      {
        "title": "Data-to-Model Distillation: Data-Efficient Learning Framework",
        "author": "Ahmad Sajedi & Samir Khaki et al., ECCV 2024",
        "github": null,
        "url": "https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6020_ECCV_2024_paper.php",
        "cite": "sajedi2024data",
        "website": null
      },
      {
        "title": "Generative Dataset Distillation Based on Self-knowledge Distillation",
        "author": "Longzhen Li & Guang Li et al., ICASSP 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2501.04202",
        "cite": "li2025generative",
        "website": null
      },
      {
        "title": "Hierarchical Features Matter: A Deep Exploration of GAN Priors for Improved Dataset Distillation",
        "author": "Xinhao Zhong & Hao Fang et al., CVPR 2025",
        "github": "https://github.com/ndhg1213/H-GLaD",
        "url": "https://arxiv.org/abs/2406.05704",
        "cite": "zhong2025hglad",
        "website": null
      }
    ],
    "_diffusion": [
      {
        "title": "Efficient Dataset Distillation via Minimax Diffusion",
        "author": "Jianyang Gu et al., CVPR 2024",
        "github": "https://github.com/vimar-gu/MinimaxDiffusion",
        "url": "https://arxiv.org/abs/2311.15529",
        "cite": "gu2024efficient",
        "website": null
      },
      {
        "title": "D4M: Dataset Distillation via Disentangled Diffusion Model",
        "author": "Duo Su & Junjie Hou et al., CVPR 2024",
        "github": "https://github.com/suduo94/D4M",
        "url": "https://arxiv.org/abs/2407.15138",
        "cite": "su2024d4m",
        "website": "https://junjie31.github.io/D4M/"
      },
      {
        "title": "Generative Dataset Distillation: Balancing Global Structure and Local Details",
        "author": "Longzhen Li & Guang Li et al., CVPR 2024 Workshop",
        "github": null,
        "url": "https://arxiv.org/abs/2404.17732",
        "cite": "li2024generative",
        "website": null
      },
      {
        "title": "Generative Dataset Distillation Based on Diffusion Models",
        "author": "Duo Su & Junjie Hou & Guang Li et al., ECCV 2024 Workshop",
        "github": "https://github.com/Guang000/Generative-Dataset-Distillation-Based-on-Diffusion-Model",
        "url": "https://arxiv.org/abs/2408.08610",
        "cite": "su2024diffusion",
        "website": null
      },
      {
        "title": "Latent Dataset Distillation with Diffusion Models",
        "author": "Brian B. Moser & Federico Raue et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2403.03881",
        "cite": "moser2024ld3m",
        "website": null
      },
      {
        "title": "Influence-Guided Diffusion for Dataset Distillation",
        "author": "Mingyang Chen et al., ICLR 2025",
        "github": "https://github.com/mchen725/DD_IGD",
        "url": "https://openreview.net/forum?id=0whx8MhysK",
        "cite": "chen2025igd",
        "website": null
      },
      {
        "title": "Taming Diffusion for Dataset Distillation with High Representativeness",
        "author": "Lin Zhao et al., ICML 2025",
        "github": "https://github.com/lin-zhao-resoLve/D3HR",
        "url": "https://arxiv.org/abs/2505.18399",
        "cite": "zhao2025d3hr",
        "website": null
      },
      {
        "title": "MGD3: Mode-Guided Dataset Distillation using Diffusion Models",
        "author": "Jeffrey A. Chan-Santiago et al., ICML 2025",
        "github": "https://github.com/jachansantiago/mode_guidance/",
        "url": "https://arxiv.org/abs/2505.18963",
        "cite": "chan-santiago2025mgd3",
        "website": "https://jachansantiago.com/mode-guided-distillation/"
      },
      {
        "title": "CaO2: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation",
        "author": "Haoxuan Wang et al., ICCV 2025",
        "github": "https://github.com/hatchetProject/CaO2",
        "url": "https://arxiv.org/abs/2506.22637",
        "cite": "wang2025cao2",
        "website": null
      },
      {
        "title": "Dataset Distillation via Vision-Language Category Prototype",
        "author": "Yawen Zou & Guang Li et al., ICCV 2025",
        "github": "https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/",
        "url": "https://arxiv.org/abs/2506.23580",
        "cite": "zou2025vlcp",
        "website": null
      },
      {
        "title": "Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling",
        "author": "Mingzhuo Li & Guang Li et al., ICCV 2025 Workshop",
        "github": "https://github.com/SumomoTaku/DiffGuideSamp",
        "url": "https://arxiv.org/abs/2507.03331",
        "cite": "li2025diff",
        "website": null
      },
      {
        "title": "Information-Guided Diffusion Sampling for Dataset Distillation",
        "author": "Linfeng Ye & Guang Li et al., 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2507.04619",
        "cite": "ye2025igds",
        "website": null
      }
    ],
    "better-optimization": [
      {
        "title": "Accelerating Dataset Distillation via Model Augmentation",
        "author": "Lei Zhang & Jie Zhang et al., CVPR 2023",
        "github": "https://github.com/ncsu-dk-lab/Acc-DD",
        "url": "https://arxiv.org/abs/2212.06152",
        "cite": "zhang2023accelerating",
        "website": null
      },
      {
        "title": "DREAM: Efficient Dataset Distillation by Representative Matching",
        "author": "Yanqing Liu & Jianyang Gu & Kai Wang et al., ICCV 2023",
        "github": "https://github.com/lyq312318224/DREAM",
        "url": "https://arxiv.org/abs/2302.14416",
        "cite": "liu2023dream",
        "website": null
      },
      {
        "title": "You Only Condense Once: Two Rules for Pruning Condensed Datasets",
        "author": "Yang He et al., NeurIPS 2023",
        "github": "https://github.com/he-y/you-only-condense-once",
        "url": "https://arxiv.org/abs/2310.14019",
        "cite": "he2023yoco",
        "website": null
      },
      {
        "title": "MIM4DD: Mutual Information Maximization for Dataset Distillation",
        "author": "Yuzhang Shang et al., NeurIPS 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2312.16627",
        "cite": "shang2023mim4dd",
        "website": null
      },
      {
        "title": "Can Pre-Trained Models Assist in Dataset Distillation?",
        "author": "Yao Lu et al., 2023",
        "github": "https://github.com/yaolu-zjut/DDInterpreter",
        "url": "lu2023pre",
        "cite": "https://arxiv.org/abs/2310.03295",
        "website": null
      },
      {
        "title": "DREAM+: Efficient Dataset Distillation by Bidirectional Representative Matching",
        "author": "Yanqing Liu & Jianyang Gu & Kai Wang et al., 2023",
        "github": "https://github.com/lyq312318224/DREAM",
        "url": "https://arxiv.org/abs/2310.15052",
        "cite": "liu2023dream+",
        "website": null
      },
      {
        "title": "Dataset Distillation in Latent Space",
        "author": "Yuxuan Duan et al., 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2311.15547",
        "cite": "duan2023latent",
        "website": null
      },
      {
        "title": "Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality",
        "author": "Xuxi Chen & Yu Yang et al., ICLR 2024",
        "github": "https://github.com/VITA-Group/ProgressiveDD",
        "url": "https://arxiv.org/abs/2310.06982",
        "cite": "chen2024vodka",
        "website": null
      },
      {
        "title": "Embarassingly Simple Dataset Distillation",
        "author": "Yunzhen Feng et al., ICLR 2024",
        "github": "https://github.com/fengyzpku/Simple_Dataset_Distillation",
        "url": "https://arxiv.org/abs/2311.07025",
        "cite": "yunzhen2024embarassingly",
        "website": null
      },
      {
        "title": "Multisize Dataset Condensation",
        "author": "Yang He et al., ICLR 2024",
        "github": "https://github.com/he-y/Multisize-Dataset-Condensation",
        "url": "https://arxiv.org/abs/2403.06075",
        "cite": "he2024mdc",
        "website": null
      },
      {
        "title": "Large Scale Dataset Distillation with Domain Shift",
        "author": "Noel Loo & Alaa Maalouf et al., ICML 2024",
        "github": "https://github.com/yolky/d3s_distillation",
        "url": "https://openreview.net/forum?id=0FWPKHMCSc",
        "cite": "loo2024d3s",
        "website": null
      },
      {
        "title": "https://github.com/silicx/GoldFromOres",
        "author": "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation",
        "github": "Yue Xu et al., ECCV 2024",
        "url": "https://arxiv.org/abs/2305.18381",
        "cite": "xu2024distill",
        "website": null
      },
      {
        "title": "Towards Model-Agnostic Dataset Condensation by Heterogeneous Models",
        "author": "Jun-Yeong Moon et al., ECCV 2024",
        "github": "https://github.com/khu-agi/hmdc",
        "url": "https://arxiv.org/abs/2409.14538",
        "cite": "moon2024hmdc",
        "website": null
      },
      {
        "title": "Teddy: Efficient Large-Scale Dataset Distillation via Taylor-Approximated Matching",
        "author": "Ruonan Yu et al., ECCV 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2410.07579",
        "cite": "yu2024teddy",
        "website": null
      },
      {
        "title": "BACON: Bayesian Optimal Condensation Framework for Dataset Distillation",
        "author": "Zheng Zhou et al., 2024",
        "github": "https://github.com/zhouzhengqd/BACON",
        "url": "https://arxiv.org/abs/2406.01112",
        "cite": "zhou2024bacon",
        "website": null
      },
      {
        "title": "Going Beyond Feature Similarity: Effective Dataset Distillation based on Class-aware Conditional Mutual Information",
        "author": "Xinhao Zhong et al., ICLR 2025",
        "github": "https://github.com/ndhg1213/CMIDD",
        "url": "https://arxiv.org/abs/2412.09945",
        "cite": "zhong2025cmi",
        "website": null
      },
      {
        "title": "Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation",
        "author": "Yanda Chen & Gongwei Chen et al., CVPR 2025",
        "github": "https://github.com/CYDaaa30/CCFS",
        "url": "https://arxiv.org/abs/2503.18872",
        "cite": "chen2025ccfs",
        "website": null
      }
    ],
    "better-understanding": [
      {
        "title": "Optimizing Millions of Hyperparameters by Implicit Differentiation",
        "author": "Jonathan Lorraine et al., AISTATS 2020",
        "github": "https://github.com/MaximeVandegar/Papers-in-100-Lines-of-Code/tree/main/Optimizing_Millions_of_Hyperparameters_by_Implicit_Differentiation",
        "url": "https://arxiv.org/abs/1911.02590",
        "cite": "lorraine2020optimizing",
        "website": null
      },
      {
        "title": "On Implicit Bias in Overparameterized Bilevel Optimization",
        "author": "Paul Vicol et al., ICML 2022",
        "github": null,
        "url": "https://proceedings.mlr.press/v162/vicol22a.html",
        "cite": "vicol2022implicit",
        "website": null
      },
      {
        "title": "On the Size and Approximation Error of Distilled Sets",
        "author": "Alaa Maalouf & Murad Tukan et al., NeurIPS 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2305.14113",
        "cite": "maalouf2023size",
        "website": null
      },
      {
        "title": "A Theoretical Study of Dataset Distillation",
        "author": "Zachary Izzo et al., NeurIPS 2023 Workshop",
        "github": null,
        "url": "https://openreview.net/forum?id=dq5QGXGxoJ",
        "cite": "izzo2023theo",
        "website": null
      },
      {
        "title": "What is Dataset Distillation Learning?",
        "author": "William Yang et al., ICML 2024",
        "github": "https://github.com/princetonvisualai/What-is-Dataset-Distillation-Learning",
        "url": "https://arxiv.org/abs/2406.04284",
        "cite": "yang2024learning",
        "website": null
      },
      {
        "title": "Mitigating Bias in Dataset Distillation",
        "author": "Justin Cui et al., ICML 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2406.06609",
        "cite": "cui2024bias",
        "website": null
      },
      {
        "title": "Dataset Distillation from First Principles: Integrating Core Information Extraction and Purposeful Learning",
        "author": "Vyacheslav Kungurtsev et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2409.01410",
        "cite": "kungurtsev2024first",
        "website": null
      },
      {
        "title": "Not All Samples Should Be Utilized Equally: Towards Understanding and Improving Dataset Distillation",
        "author": "Shaobo Wang et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2408.12483",
        "cite": "wang2024samples",
        "website": null
      },
      {
        "title": "Dataset Distillation as Pushforward Optimal Quantization",
        "author": "Hongye Tan et al., 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2501.07681",
        "cite": "tan2025optimal",
        "website": null
      }
    ],
    "label": [
      {
        "title": "Flexible Dataset Distillation: Learn Labels Instead of Images",
        "author": "Ondrej Bohdal et al., NeurIPS 2020 Workshop",
        "github": "https://github.com/ondrejbohdal/label-distillation",
        "url": "https://arxiv.org/abs/2006.08572",
        "cite": "bohdal2020flexible",
        "website": null
      },
      {
        "title": "Soft-Label Dataset Distillation and Text Dataset Distillation",
        "author": "Ilia Sucholutsky et al., IJCNN 2021",
        "github": "https://github.com/ilia10000/dataset-distillation",
        "url": "https://arxiv.org/abs/1910.02551",
        "cite": "sucholutsky2021soft",
        "website": null
      },
      {
        "title": "A Label is Worth a Thousand Images in Dataset Distillation",
        "author": "Tian Qin et al., NeurIPS 2024",
        "github": "https://github.com/sunnytqin/no-distillation",
        "url": "https://arxiv.org/abs/2406.10485",
        "cite": "qin2024label",
        "website": null
      },
      {
        "title": "Are Large-scale Soft Labels Necessary for Large-scale Dataset Distillation?",
        "author": "Lingao Xiao et al., NeurIPS 2024",
        "github": "https://github.com/he-y/soft-label-pruning-for-dataset-distillation",
        "url": "https://arxiv.org/abs/2410.15919",
        "cite": "xiao2024soft",
        "website": null
      },
      {
        "title": "DRUPI: Dataset Reduction Using Privileged Information",
        "author": "Shaobo Wang et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2410.01611",
        "cite": "wang2024drupi",
        "website": null
      },
      {
        "title": "Label-Augmented Dataset Distillation",
        "author": "Seoungyoon Kang & Youngsun Lim et al., WACV 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2409.16239",
        "cite": "kang2024label",
        "website": null
      },
      {
        "title": "GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost",
        "author": "Xinyi Shang & Peng Sun et al., ICLR 2025",
        "github": "https://github.com/LINs-lab/GIFT",
        "url": "https://arxiv.org/abs/2405.14736",
        "cite": "shang2025gift",
        "website": null
      },
      {
        "title": "Heavy Labels Out! Dataset Distillation with Label Space Lightening",
        "author": "Ruonan Yu et al., ICCV 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2408.08201",
        "cite": "yu2025helio",
        "website": null
      }
    ],
    "quantization": [
      {
        "title": "Dataset Quantization",
        "author": "Daquan Zhou & Kai Wang & Jianyang Gu et al., ICCV 2023",
        "github": "https://github.com/magic-research/Dataset_Quantization",
        "url": "https://arxiv.org/abs/2308.10524",
        "cite": "zhou2023dataset",
        "website": null
      },
      {
        "title": "Dataset Quantization with Active Learning based Adaptive Sampling",
        "author": "Zhenghao Zhao et al., ECCV 2024",
        "github": "https://github.com/ichbill/DQAS",
        "url": "https://arxiv.org/abs/2407.07268",
        "cite": "zhao2024dqas",
        "website": null
      },
      {
        "title": "Adaptive Dataset Quantization",
        "author": "Muquan Li et al., AAAI 2025",
        "github": "https://github.com/SLGSP/ADQ",
        "url": "https://www.arxiv.org/abs/2412.16895",
        "cite": "li2025adq",
        "website": null
      }
    ],
    "decouple": [
      {
        "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective",
        "author": "Zeyuan Yin & Zhiqiang Shen et al., NeurIPS 2023",
        "github": "https://github.com/VILA-Lab/SRe2L/tree/main/SRe2L",
        "url": "https://arxiv.org/abs/2306.13092",
        "cite": "yin2023sre2l",
        "website": "https://zeyuanyin.github.io/projects/SRe2L/"
      },
      {
        "title": "Dataset Distillation via Curriculum Data Synthesis in Large Data Era",
        "author": "Zeyuan Yin et al., TMLR 2024",
        "github": "https://github.com/VILA-Lab/SRe2L/tree/main/CDA",
        "url": "https://arxiv.org/abs/2311.18838",
        "cite": "yin2024cda",
        "website": null
      },
      {
        "title": "Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching",
        "author": "Shitong Shao et al., CVPR 2024",
        "github": "https://github.com/shaoshitong/G_VBSM_Dataset_Condensation",
        "url": "https://arxiv.org/abs/2311.17950",
        "cite": "shao2024gvbsm",
        "website": null
      },
      {
        "title": "On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm",
        "author": "Peng Sun et al., CVPR 2024",
        "github": "https://github.com/LINs-lab/RDED",
        "url": "https://arxiv.org/abs/2312.03526",
        "cite": "sun2024rded",
        "website": null
      },
      {
        "title": "Information Compensation: A Fix for Any-scale Dataset Distillation",
        "author": "Peng Sun et al., ICLR 2024 Workshop",
        "github": null,
        "url": "https://openreview.net/forum?id=2SnmKd1JK4",
        "cite": "sun2024lic",
        "website": null
      },
      {
        "title": "Elucidating the Design Space of Dataset Condensation",
        "author": "Shitong Shao et al., NeurIPS 2024",
        "github": "https://github.com/shaoshitong/EDC",
        "url": "https://arxiv.org/abs/2404.13733",
        "cite": "shao2024edc",
        "website": null
      },
      {
        "title": "Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment",
        "author": "Jiawei Du et al., NeurIPS 2024",
        "github": "https://github.com/AngusDujw/Diversity-Driven-Synthesis",
        "url": "du2024diversity",
        "cite": "https://arxiv.org/abs/2409.17612",
        "website": null
      },
      {
        "title": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator",
        "author": "Xin Zhang et al., ICLR 2025",
        "github": "https://github.com/zhangxin-xd/UFC",
        "url": "https://arxiv.org/abs/2408.06927",
        "cite": "zhang2025infer",
        "website": null
      },
      {
        "title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation",
        "author": "Minh-Tuan Tran et al., CVPR 2025",
        "github": "https://github.com/tmtuan1307/NRR-DD",
        "url": "https://arxiv.org/abs/2503.18267",
        "cite": "tran2025nrrdd",
        "website": null
      },
      {
        "title": "Enhancing Dataset Distillation via Non-Critical Region Refinement",
        "author": "Zhiqiang Shen & Ammar Sherif et al., CVPR 2025",
        "github": "https://github.com/VILA-Lab/DELT",
        "url": "https://arxiv.org/abs/2411.19946",
        "cite": "shen2025delt",
        "website": null
      },
      {
        "title": "Curriculum Dataset Distillation",
        "author": "Zhiheng Ma & Anjia Cao et al., TIP 2025",
        "github": "https://github.com/MIV-XJTU/CUDD",
        "url": "https://arxiv.org/abs/2405.09150",
        "cite": "ma2025cudd",
        "website": null
      },
      {
        "title": "FocusDD: Real-World Scene Infusion for Robust Dataset Distillation",
        "author": "Youbin Hu et al., 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2501.06405",
        "cite": "hu2025focusdd",
        "website": null
      },
      {
        "title": "Dataset Distillation via Committee Voting",
        "author": "Jiacheng Cui et al., 2025",
        "github": "https://github.com/Jiacheng8/CV-DD",
        "url": "https://arxiv.org/abs/2501.07575",
        "cite": "cui2025cvdd",
        "website": null
      }
    ],
    "multimodal": [
      {
        "title": "Vision-Language Dataset Distillation",
        "author": "Xindi Wu et al., TMLR 2024",
        "github": "https://github.com/princetonvisualai/multimodal_dataset_distillation",
        "url": "https://arxiv.org/abs/2308.07545",
        "cite": "wu2024multi",
        "website": "https://princetonvisualai.github.io/multimodal_dataset_distillation/"
      },
      {
        "title": "Low-Rank Similarity Mining for Multimodal Dataset Distillation",
        "author": "Yue Xu et al., ICML 2024",
        "github": "https://github.com/silicx/LoRS_Distill",
        "url": "https://arxiv.org/abs/2406.03793",
        "cite": "xu2024lors",
        "website": null
      },
      {
        "title": "Audio-Visual Dataset Distillation",
        "author": "Saksham Singh Kushwaha et al., TMLR 2024",
        "github": "https://github.com/sakshamsingh1/AVDD",
        "url": "https://openreview.net/forum?id=IJlbuSrXmk",
        "cite": "kush2024avdd",
        "website": null
      },
      {
        "title": "Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation",
        "author": "Xin Zhang et al., 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2505.14705",
        "cite": "zhang2025mdd",
        "website": null
      }
    ],
    "self": [
      {
        "title": "Dong Bok Lee & Seanie Lee et al., ICLR 2024",
        "author": "Self-Supervised Dataset Distillation for Transfer Learning",
        "github": "https://github.com/db-Lee/selfsup_dd",
        "url": "https://arxiv.org/abs/2310.06511",
        "cite": "lee2024self",
        "website": null
      },
      {
        "title": "Efficiency for Free: Ideal Data Are Transportable Representations",
        "author": "Peng Sun et al., NeurIPS 2024",
        "github": "https://github.com/LINs-lab/ReLA",
        "url": "https://arxiv.org/abs/2405.14669",
        "cite": "sun2024rela",
        "website": null
      },
      {
        "title": "Self-supervised Dataset Distillation: A Good Compression Is All You Need",
        "author": "Muxin Zhou et al., 2024",
        "github": "https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/",
        "url": "https://arxiv.org/abs/2404.07976",
        "cite": "zhou2024self",
        "website": null
      },
      {
        "title": "Dataset Distillation via Knowledge Distillation: Towards Efficient Self-Supervised Pre-Training of Deep Networks",
        "author": "Siddharth Joshi et al., ICLR 2025",
        "github": "https://github.com/jiayini1119/MKDT",
        "url": "https://arxiv.org/abs/2410.02116",
        "cite": "joshi2025kd",
        "website": null
      },
      {
        "title": "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation",
        "author": "Sheng-Feng Yu et al., ICLR 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2507.05914",
        "cite": "yu2025self",
        "website": null
      }
    ],
    "uni": [
      {
        "title": "Towards Universal Dataset Distillation via Task-Driven Diffusion",
        "author": "Ding Qi et al., CVPR 2025",
        "github": null,
        "url": "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.html",
        "cite": "qi2025unidd",
        "website": null
      }
    ],
    "benchmark": [
      {
        "title": "DC-BENCH: Dataset Condensation Benchmark",
        "author": "Justin Cui et al., NeurIPS 2022",
        "github": "https://github.com/justincui03/dc_benchmark",
        "url": "https://arxiv.org/abs/2207.09639",
        "cite": "cui2022dc",
        "website": "https://dc-bench.github.io/"
      },
      {
        "title": "A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness",
        "author": "Zongxiong Chen & Jiahui Geng et al., 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2305.03355",
        "cite": "chen2023study",
        "website": null
      },
      {
        "title": "DD-RobustBench: An Adversarial Robustness Benchmark for Dataset Distillation",
        "author": "Yifan Wu et al., TIP 2025",
        "github": "https://github.com/FredWU-HUST/DD-RobustBench",
        "url": "https://arxiv.org/abs/2403.13322",
        "cite": "wu2025robust",
        "website": null
      },
      {
        "title": "BEARD: Benchmarking the Adversarial Robustness for Dataset Distillation",
        "author": "Zheng Zhou et al., 2024",
        "github": "https://github.com/zhouzhengqd/BEARD/",
        "url": "https://arxiv.org/abs/2411.09265",
        "cite": "zhou2024beard",
        "website": "https://beard-leaderboard.github.io/"
      }
    ],
    "survey": [
      {
        "title": "Data Distillation: A Survey",
        "author": "Noveen Sachdeva et al., TMLR 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2301.04272",
        "cite": "sachdeva2023survey",
        "website": null
      },
      {
        "title": "A Survey on Dataset Distillation: Approaches, Applications and Future Directions",
        "author": "Jiahui Geng & Zongxiong Chen et al., IJCAI 2023",
        "github": "https://github.com/Guang000/Awesome-Dataset-Distillation",
        "url": "https://arxiv.org/abs/2305.01975",
        "cite": "geng2023survey",
        "website": null
      },
      {
        "title": "A Comprehensive Survey to Dataset Distillation",
        "author": "Shiye Lei et al., TPAMI 2023",
        "github": "https://github.com/Guang000/Awesome-Dataset-Distillation",
        "url": "https://arxiv.org/abs/2301.05603",
        "cite": "nooralinejad2022pranc",
        "website": null
      },
      {
        "title": "Dataset Distillation: A Comprehensive Review",
        "author": "Ruonan Yu & Songhua Liu et al., TPAMI 2023",
        "github": "https://github.com/Guang000/Awesome-Dataset-Distillation",
        "url": "https://arxiv.org/abs/2301.07014",
        "cite": "yu2023review",
        "website": null
      },
      {
        "title": "The Evolution of Dataset Distillation: Toward Scalable and Generalizable Solutions",
        "author": "Ping Liu et al., 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2502.05673",
        "cite": "liu2025survey",
        "website": null
      }
    ],
    "phd": [
      {
        "title": "Data-efficient Neural Network Training with Dataset Condensation",
        "author": "Bo Zhao, The University of Edinburgh 2023",
        "github": null,
        "url": "https://era.ed.ac.uk/handle/1842/39756",
        "cite": "zhao2023thesis",
        "website": null
      }
    ],
    "workshop": [
      {
        "title": "1st CVPR Workshop on Dataset Distillation",
        "author": "Saeed Vahidian et al., CVPR 2024",
        "github": null,
        "url": null,
        "cite": null,
        "website": "https://sites.google.com/view/dd-cvpr2024/home"
      }
    ],
    "challenge": [
      {
        "title": "The First Dataset Distillation Challenge",
        "author": "Kai Wang & Ahmad Sajedi et al., ECCV 2024",
        "github": "https://github.com/DataDistillation/ECCV2024-Dataset-Distillation-Challenge",
        "url": null,
        "cite": null,
        "website": "https://www.dd-challenge.com/"
      }
    ],
    "ranking": [
      {
        "title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation",
        "author": "Zekai Li & Xinhao Zhong et al., 2025",
        "github": "https://github.com/NUS-HPC-AI-Lab/DD-Ranking",
        "url": "https://arxiv.org/abs/2505.13300",
        "cite": "li2025ranking",
        "website": "https://nus-hpc-ai-lab.github.io/DD-Ranking/"
      }
    ]
  },
  "applications": {
    "continual": [
      {
        "title": "Reducing Catastrophic Forgetting with Learning on Synthetic Data",
        "author": "Wojciech Masarczyk et al., CVPR 2020 Workshop",
        "github": null,
        "url": "https://arxiv.org/abs/2004.14046",
        "cite": "masarczyk2020reducing",
        "website": null
      },
      {
        "title": "Condensed Composite Memory Continual Learning",
        "author": "Felix Wiewel et al., IJCNN 2021",
        "github": "https://github.com/FelixWiewel/CCMCL",
        "url": "https://arxiv.org/abs/2102.09890",
        "cite": "wiewel2021soft",
        "website": null
      },
      {
        "title": "Distilled Replay: Overcoming Forgetting through Synthetic Samples",
        "author": "Andrea Rosasco et al., IJCAI 2021 Workshop",
        "github": "https://github.com/andrearosasco/DistilledReplay",
        "url": "https://arxiv.org/abs/2103.15851",
        "cite": "rosasco2021distilled",
        "website": null
      },
      {
        "title": "Sample Condensation in Online Continual Learning",
        "author": "Mattia Sangermano et al., IJCNN 2022",
        "github": "https://github.com/MattiaSangermano/OLCGM",
        "url": "https://arxiv.org/abs/2206.11849",
        "cite": "sangermano2022sample",
        "website": null
      },
      {
        "title": "An Efficient Dataset Condensation Plugin and Its Application to Continual Learning",
        "author": "Enneng Yang et al., NeurIPS 2023",
        "github": "https://github.com/EnnengYang/An-Efficient-Dataset-Condensation-Plugin",
        "url": "https://openreview.net/forum?id=Murj6wcjRw",
        "cite": "yang2023efficient",
        "website": null
      },
      {
        "title": "Summarizing Stream Data for Memory-Restricted Online Continual Learning",
        "author": "Jianyang Gu et al., AAAI 2024",
        "github": "https://github.com/vimar-gu/SSD",
        "url": "https://arxiv.org/abs/2305.16645",
        "cite": "gu2024ssd",
        "website": null
      }
    ],
    "privacy": [
      {
        "title": "Privacy for Free: How does Dataset Condensation Help Privacy?",
        "author": "Tian Dong et al., ICML 2022",
        "github": null,
        "url": "https://arxiv.org/abs/2206.00240",
        "cite": "dong2022privacy",
        "website": null
      },
      {
        "title": "https://github.com/DingfanChen/Private-Set",
        "author": "Private Set Generation with Discriminative Information",
        "github": "Dingfan Chen et al., NeurIPS 2022",
        "url": "https://arxiv.org/abs/2211.04446",
        "cite": "chen2022privacy",
        "website": null
      },
      {
        "title": "No Free Lunch in \"Privacy for Free: How does Dataset Condensation Help Privacy\"",
        "author": "Nicholas Carlini et al., 2022",
        "github": null,
        "url": "https://arxiv.org/abs/2209.14987",
        "cite": "carlini2022no",
        "website": null
      },
      {
        "title": "Backdoor Attacks Against Dataset Distillation",
        "author": "https://github.com/liuyugeng/baadd",
        "github": "Yugeng Liu et al., NDSS 2023",
        "url": "https://arxiv.org/abs/2301.01197",
        "cite": "liu2023backdoor",
        "website": null
      },
      {
        "title": "Differentially Private Kernel Inducing Points (DP-KIP) for Privacy-preserving Data Distillation",
        "author": "Margarita Vinaroz et al., 2023",
        "github": "https://github.com/dpclip/dpclip",
        "url": "https://arxiv.org/abs/2301.13389",
        "cite": "vinaroz2023dpkip",
        "website": null
      },
      {
        "title": "Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation",
        "author": "Noel Loo et al., ICLR 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2302.01428",
        "cite": "loo2024attack",
        "website": null
      },
      {
        "title": "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective",
        "author": "Ming-Yu Chung et al., ICLR 2024",
        "github": null,
        "url": "chung2024backdoor",
        "cite": "https://arxiv.org/abs/2311.16646",
        "website": null
      },
      {
        "title": "Differentially Private Dataset Condensation",
        "author": "Zheng et al., NDSS 2024 Workshop",
        "github": null,
        "url": "https://www.ndss-symposium.org/ndss-paper/auto-draft-542/",
        "cite": "zheng2024differentially",
        "website": null
      },
      {
        "title": "Adaptive Backdoor Attacks Against Dataset Distillation for Federated Learning",
        "author": "Ze Chai et al., ICC 2024",
        "github": null,
        "url": "https://ieeexplore.ieee.org/abstract/document/10622462?casa_token=tHyZ-Pz7DpUAAAAA:vmCYI4cUcKzMluUsASHhIhr0CvBkjzBR-0N7REVj7aFN5hT5TinQTpSEsE0Bo3Fl8auh52Fipm_v",
        "cite": "chai2024backdoor",
        "website": null
      }
    ],
    "medical": [
      {
        "title": "Soft-Label Anonymous Gastric X-ray Image Distillation",
        "author": "Guang Li et al., ICIP 2020",
        "github": "https://github.com/Guang000/dataset-distillation",
        "url": "https://arxiv.org/abs/2104.02857",
        "cite": "li2020soft",
        "website": null
      },
      {
        "title": "Compressed Gastric Image Generation Based on Soft-Label Dataset Distillation for Medical Data Sharing",
        "author": "Guang Li et al., CMPB 2022",
        "github": "https://github.com/Guang000/dataset-distillation",
        "url": "https://arxiv.org/abs/2209.14635",
        "cite": "li2022compressed",
        "website": null
      },
      {
        "title": "Dataset Distillation for Medical Dataset Sharing",
        "author": "Guang Li et al., AAAI 2023 Workshop",
        "github": "https://github.com/Guang000/mtt-distillation",
        "url": "https://r2hcai.github.io/AAAI-23/pages/accepted-papers.html",
        "cite": "li2023sharing",
        "website": null
      },
      {
        "title": "Communication-Efficient Federated Skin Lesion Classification with Generalizable Dataset Distillation",
        "author": "Yuchen Tian & Jiacheng Wang et al., MICCAI 2023 Workshop",
        "github": null,
        "url": "https://link.springer.com/chapter/10.1007/978-3-031-47401-9_2",
        "cite": "tian2023gdd",
        "website": null
      },
      {
        "title": "Importance-Aware Adaptive Dataset Distillation",
        "author": "Guang Li et al., NN 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2401.15863",
        "cite": "li2024iadd",
        "website": null
      },
      {
        "title": "Image Distillation for Safe Data Sharing in Histopathology",
        "author": "Zhe Li et al., MICCAI 2024",
        "github": "https://github.com/ZheLi2020/InfoDist",
        "url": "https://arxiv.org/abs/2406.13536",
        "cite": "li2024infodist",
        "website": null
      },
      {
        "title": "MedSynth: Leveraging Generative Model for Healthcare Data Sharing",
        "author": "Renuga Kanagavelu et al., MICCAI 2024",
        "github": null,
        "url": "https://link.springer.com/chapter/10.1007/978-3-031-72390-2_61",
        "cite": "kanagavelu2024medsynth",
        "website": null
      },
      {
        "title": "Progressive Trajectory Matching for Medical Dataset Distillation",
        "author": "Zhen Yu et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2403.13469",
        "cite": "yu2024progressive",
        "website": null
      },
      {
        "title": "Dataset Distillation in Medical Imaging: A Feasibility Study",
        "author": "Muyang Li et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2407.14429",
        "cite": "li2024medical",
        "website": null
      },
      {
        "title": "Dataset Distillation for Histopathology Image Classification",
        "author": "Cong Cong et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2408.09709",
        "cite": "cong2024dataset",
        "website": null
      },
      {
        "title": "FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation",
        "author": "Haolong Jin et al., MICCAI 2025",
        "github": "https://github.com/f1oNae/FedWSIDD",
        "url": "https://arxiv.org/abs/2506.15365",
        "cite": "jin2025fedwsidd",
        "website": null
      }
    ],
    "federated": [
      {
        "title": "Federated Learning via Synthetic Data",
        "author": "Jack Goetz et al., 2020",
        "github": null,
        "url": "https://arxiv.org/abs/2008.04489",
        "cite": "goetz2020federated",
        "website": null
      },
      {
        "title": "Distilled One-Shot Federated Learning",
        "author": "Yanlin Zhou et al., 2020",
        "github": null,
        "url": "https://arxiv.org/abs/2009.07999",
        "cite": "zhou2020distilled",
        "website": null
      },
      {
        "title": "DENSE: Data-Free One-Shot Federated Learning",
        "author": "Jie Zhang & Chen Chen et al., NeurIPS 2022",
        "github": "https://github.com/zj-jayzhang/DENSE",
        "url": "https://arxiv.org/abs/2112.12371",
        "cite": "zhang2022dense",
        "website": null
      },
      {
        "title": "FedSynth: Gradient Compression via Synthetic Data in Federated Learning",
        "author": "Shengyuan Hu et al., 2022",
        "github": null,
        "url": "https://arxiv.org/abs/2204.01273",
        "cite": "hu2022fedsynth",
        "website": null
      },
      {
        "title": "Meta Knowledge Condensation for Federated Learning",
        "author": "Ping Liu et al., ICLR 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2209.14851",
        "cite": "liu2023meta",
        "website": null
      },
      {
        "title": "DYNAFED: Tackling Client Data Heterogeneity with Global Dynamics",
        "author": "Renjie Pi et al., CVPR 2023",
        "github": "https://github.com/pipilurj/dynafed",
        "url": "https://arxiv.org/abs/2211.10878",
        "cite": "pi2023dynafed",
        "website": null
      },
      {
        "title": "FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning",
        "author": "Yuanhao Xiong & Ruochen Wang et al., CVPR 2023",
        "github": "https://github.com/anonymifish/fed-distribution-matching",
        "url": "https://arxiv.org/abs/2207.09653",
        "cite": "xiong2023feddm",
        "website": null
      },
      {
        "title": "Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments",
        "author": "Rui Song et al., IJCNN 2023",
        "github": "https://github.com/rruisong/fedd3",
        "url": "https://arxiv.org/abs/2208.11311",
        "cite": "song2023federated",
        "website": null
      },
      {
        "title": "FedLAP-DP: Federated Learning by Sharing Differentially Private Loss Approximations",
        "author": "Hui-Po Wang et al., 2023",
        "github": "https://github.com/a514514772/fedlap-dp",
        "url": "https://arxiv.org/abs/2302.01068",
        "cite": "wang2023fed",
        "website": null
      },
      {
        "title": "Federated Virtual Learning on Heterogeneous Data with Local-global Distillation",
        "author": "Chun-Yin Huang et al., 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2303.02278",
        "cite": "huang2023federated",
        "website": null
      },
      {
        "title": "An Aggregation-Free Federated Learning for Tackling Data Heterogeneity",
        "author": "Yuan Wang et al., CVPR 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2404.18962",
        "cite": "wang2024fed",
        "website": null
      },
      {
        "title": "Overcoming Data and Model Heterogeneities in Decentralized Federated Learning via Synthetic Anchors",
        "author": "Chun-Yin Huang et al., ICML 2024",
        "github": "https://github.com/ubc-tea/DESA",
        "url": "https://arxiv.org/abs/2405.11525",
        "cite": "huang2024desa",
        "website": null
      },
      {
        "title": "DCFL: Non-IID Awareness Dataset Condensation Aided Federated Learning",
        "author": "Xingwang Wang et al., IJCNN 2024",
        "github": "https://github.com/JLUssh/DCFL",
        "url": "https://ieeexplore.ieee.org/document/10650791",
        "cite": "wang2024dcfl",
        "website": null
      },
      {
        "title": "Unlocking the Potential of Federated Learning: The Symphony of Dataset Distillation via Deep Generative Latents",
        "author": "Yuqi Jia & Saeed Vahidian et al., ECCV 2024",
        "github": "https://github.com/FedDG23/FedDG-main",
        "url": "https://arxiv.org/abs/2312.01537",
        "cite": "jia2024feddg",
        "website": null
      },
      {
        "title": "One-Shot Collaborative Data Distillation",
        "author": "William Holland et al., ECAI 2024",
        "github": "https://github.com/rayneholland/CollabDM",
        "url": "https://arxiv.org/abs/2408.02266",
        "cite": "holland2024one",
        "website": null
      },
      {
        "title": "FedVCK: Non-IID Robust and Communication-Efficient Federated Learning via Valuable Condensed Knowledge for Medical Image Analysis",
        "author": "Guochen Yan et al., AAAI 2025",
        "github": "https://github.com/Youth-49/FedVCK_2024",
        "url": "https://arxiv.org/abs/2412.18557",
        "cite": "yan2025fedvck",
        "website": null
      }
    ],
    "graph-neural": [
      {
        "title": "Graph Condensation for Graph Neural Networks",
        "author": "Wei Jin et al., ICLR 2022",
        "github": "https://github.com/HIPS/hypergrad",
        "url": "https://arxiv.org/abs/2110.07580",
        "cite": "jin2022graph",
        "website": null
      },
      {
        "title": "Condensing Graphs via One-Step Gradient Matching",
        "author": "Wei Jin et al., KDD 2022",
        "github": "https://github.com/amazon-research/DosCond",
        "url": "https://arxiv.org/abs/2206.07746",
        "cite": "jin2022condensing",
        "website": null
      },
      {
        "title": "Graph Condensation via Receptive Field Distribution Matching",
        "author": "Mengyang Liu et al., 2022",
        "github": null,
        "url": "https://arxiv.org/abs/2206.13697",
        "cite": "liu2022graph",
        "website": null
      },
      {
        "title": "Kernel Ridge Regression-Based Graph Dataset Distillation",
        "author": "Zhe Xu et al., KDD 2023",
        "github": "https://github.com/pricexu/KIDD",
        "url": "https://dl.acm.org/doi/10.1145/3580305.3599398",
        "cite": "xu2023kidd",
        "website": null
      },
      {
        "title": "Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data",
        "author": "Xin Zheng et al., NeurIPS 2023",
        "github": "https://github.com/amanda-zheng/sfgc",
        "url": "https://arxiv.org/abs/2306.02664",
        "cite": "zheng2023sfgc",
        "website": null
      },
      {
        "title": "Does Graph Distillation See Like Vision Dataset Counterpart?",
        "author": "Beining Yang & Kai Wang et al., NeurIPS 2023",
        "github": "https://github.com/RingBDStack/SGDD",
        "url": "https://arxiv.org/abs/2310.09192",
        "cite": "yang2023sgdd",
        "website": null
      },
      {
        "title": "CaT: Balanced Continual Graph Learning with Graph Condensation",
        "author": "Yilun Liu et al., ICDM 2023",
        "github": "https://github.com/superallen13/CaT-CGL",
        "url": "https://arxiv.org/abs/2309.09455",
        "cite": "liu2023cat",
        "website": null
      },
      {
        "title": "Mirage: Model-Agnostic Graph Distillation for Graph Classification",
        "author": "Mridul Gupta & Sahil Manchanda et al., ICLR 2024",
        "github": "https://github.com/frigategnn/Mirage",
        "url": "https://arxiv.org/abs/2310.09486",
        "cite": "gupta2024mirage",
        "website": null
      },
      {
        "title": "Graph Distillation with Eigenbasis Matching",
        "author": "Yang Liu & Deyu Bo et al., ICML 2024",
        "github": "https://github.com/liuyang-tian/GDEM",
        "url": "https://arxiv.org/abs/2310.09202",
        "cite": "liu2024gdem",
        "website": null
      },
      {
        "title": "Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching",
        "author": "Yuchen Zhang & Tianle Zhang & Kai Wang et al., ICML 2024",
        "github": "https://github.com/nus-hpc-ai-lab/geom",
        "url": "https://arxiv.org/abs/2402.05011",
        "cite": "zhang2024geom",
        "website": null
      },
      {
        "title": "Graph Data Condensation via Self-expressive Graph Structure Reconstruction",
        "author": "Zhanyu Liu & Chaolv Zeng et al., KDD 2024",
        "github": "https://github.com/zclzcl0223/GCSR",
        "url": "https://arxiv.org/abs/2403.07294",
        "cite": "liu2024gcsr",
        "website": null
      },
      {
        "title": "Two Trades is not Baffled: Condensing Graph via Crafting Rational Gradient Matching",
        "author": "Tianle Zhang & Yuchen Zhang & Kai Wang et al., 2024",
        "github": "https://github.com/nus-hpc-ai-lab/ctrl",
        "url": "https://arxiv.org/abs/2402.04924",
        "cite": "zhang2024ctrl",
        "website": null
      }
    ],
    "_g-survey": [
      {
        "title": "A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation",
        "author": "Mohammad Hashemi et al., IJCAI 2024",
        "github": "https://github.com/Emory-Melody/awesome-graph-reduction",
        "url": "https://arxiv.org/abs/2402.03358",
        "cite": "hashemi2024awesome",
        "website": null
      },
      {
        "title": "A Survey on Graph Condensation",
        "author": "Hongjia Xu et al., 2024",
        "github": "https://github.com/Frostland12138/Awesome-Graph-Condensation",
        "url": "xu2024survey",
        "cite": "https://arxiv.org/abs/2402.02000",
        "website": null
      },
      {
        "title": "Graph Condensation: A Survey",
        "author": "Xinyi Gao et al., TKDE 2025",
        "github": "https://github.com/xygaog/graph-condensation-papers",
        "url": "https://arxiv.org/abs/2401.11720",
        "cite": "gao2025graph",
        "website": null
      }
    ],
    "_g-benchmark": [
      {
        "title": "GC-Bench: An Open and Unified Benchmark for Graph Condensation",
        "author": "Qingyun Sun & Ziying Chen et al., 2024",
        "github": "https://github.com/RingBDStack/GC-Bench",
        "url": "https://arxiv.org/abs/2407.00615",
        "cite": "sun2024gcbench",
        "website": null
      },
      {
        "title": "GCondenser: Benchmarking Graph Condensation",
        "author": "Yilun Liu et al., 2024",
        "github": "https://github.com/superallen13/GCondenser",
        "url": "https://arxiv.org/abs/2405.14246",
        "cite": "liu2024gcondenser",
        "website": null
      },
      {
        "title": "GC-Bench: A Benchmark Framework for Graph Condensation with New Insights",
        "author": "Shengbo Gong & Juntong Ni et al., 2024",
        "github": "https://github.com/Emory-Melody/GraphSlim",
        "url": "https://arxiv.org/abs/2406.16715",
        "cite": "gong2024graphslim",
        "website": null
      }
    ],
    "neural-architecture": [
      {
        "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data",
        "author": "Felipe Petroski Such et al., ICML 2020",
        "github": null,
        "url": "such2020generative",
        "cite": "https://arxiv.org/abs/1912.07768",
        "website": null
      },
      {
        "title": "Learning to Generate Synthetic Training Data using Gradient Matching and Implicit Differentiation",
        "author": "Dmitry Medvedev et al., AIST 2021",
        "github": "https://github.com/dm-medvedev/efficientdistillation",
        "url": "https://arxiv.org/abs/2203.08559",
        "cite": "medvedev2021tabular",
        "website": null
      },
      {
        "title": "Calibrated Dataset Condensation for Faster Hyperparameter Search",
        "author": "Mucong Ding et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2405.17535",
        "cite": "ding2024hcdc",
        "website": null
      }
    ],
    "fashion": [
      {
        "title": "Wearable ImageNet: Synthesizing Tileable Textures via Dataset Distillation",
        "author": "George Cazenavette et al., CVPR 2022 Workshop",
        "github": "https://github.com/georgecazenavette/mtt-distillation",
        "url": "https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Cazenavette_Wearable_ImageNet_Synthesizing_Tileable_Textures_via_Dataset_Distillation_CVPRW_2022_paper.html",
        "cite": "cazenavette2022textures",
        "website": "https://georgecazenavette.github.io/mtt-distillation/"
      },
      {
        "title": "Learning from Designers: Fashion Compatibility Analysis Via Dataset Distillation",
        "author": "Yulan Chen et al., ICIP 2022",
        "github": null,
        "url": "https://ieeexplore.ieee.org/document/9897234",
        "cite": "chen2022fashion",
        "website": null
      },
      {
        "title": "Galaxy Dataset Distillation with Self-Adaptive Trajectory Matching",
        "author": "Haowen Guan et al., NeurIPS 2023 Workshop",
        "github": "https://github.com/HaowenGuan/Galaxy-Dataset-Distillation",
        "url": "https://arxiv.org/abs/2311.17967",
        "cite": "guan2023galaxy",
        "website": null
      }
    ],
    "recommender": [
      {
        "title": "Infinite Recommendation Networks: A Data-Centric Approach",
        "author": "Noveen Sachdeva et al., NeurIPS 2022",
        "github": "https://github.com/noveens/distill_cf",
        "url": "https://arxiv.org/abs/2206.02626",
        "cite": "sachdeva2022data",
        "website": null
      },
      {
        "title": "Gradient Matching for Categorical Data Distillation in CTR Prediction",
        "author": "Chen Wang et al., RecSys 2023",
        "github": null,
        "url": "https://dl.acm.org/doi/10.1145/3604915.3608769",
        "cite": "wang2023cgm",
        "website": null
      },
      {
        "title": "TD3: Tucker Decomposition Based Dataset Distillation Method for Sequential Recommendation]",
        "author": "Jiaqing Zhang et al., WWW 2025",
        "github": "https://github.com/USTC-StarTeam/TD3",
        "url": "https://arxiv.org/abs/2502.02854",
        "cite": "zhang2025td3",
        "website": null
      }
    ],
    "blackbox": [
      {
        "title": "Bidirectional Learning for Offline Infinite-width Model-based Optimization",
        "author": "Can Chen et al., NeurIPS 2022",
        "github": "https://github.com/ggchen1997/bdi",
        "url": "https://arxiv.org/abs/2209.07507",
        "cite": "chen2022bidirectional",
        "website": null
      },
      {
        "title": "Bidirectional Learning for Offline Model-based Biological Sequence Design",
        "author": "Can Chen et al., ICML 2023",
        "github": "https://github.com/GGchen1997/BIB-ICML2023-Submission",
        "url": "https://arxiv.org/abs/2301.02931",
        "cite": "chen2023bidirectional",
        "website": null
      }
    ],
    "robustness": [
      {
        "title": "Can We Achieve Robustness from Data Alone?",
        "author": "Nikolaos Tsilivis et al., ICML 2022 Workshop",
        "github": null,
        "url": "https://arxiv.org/abs/2207.11727",
        "cite": "tsilivis2022robust",
        "website": null
      },
      {
        "title": "Towards Robust Dataset Learning",
        "author": "Yihan Wu et al., 2022",
        "github": null,
        "url": "https://arxiv.org/abs/2211.10752",
        "cite": "wu2022towards",
        "website": null
      },
      {
        "title": "Rethinking Data Distillation: Do Not Overlook Calibration",
        "author": "Dongyao Zhu et al., ICCV 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2307.12463",
        "cite": "zhu2023calibration",
        "website": null
      },
      {
        "title": "Towards Trustworthy Dataset Distillation",
        "author": "Shijie Ma et al., PR 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2307.09165",
        "cite": "ma2024trustworthy",
        "website": null
      },
      {
        "title": "Towards Adversarially Robust Dataset Distillation by Curvature Regularization",
        "author": "Eric Xue et al., AAAI 2025",
        "github": "https://github.com/yumozi/GUARD",
        "url": "https://arxiv.org/abs/2403.10045",
        "cite": "xue2025robust",
        "website": null
      },
      {
        "title": "Group Distributionally Robust Dataset Distillation with Risk Minimization",
        "author": "Saeed Vahidian & Mingyu Wang & Jianyang Gu et al., ICLR 2025",
        "github": "https://github.com/Mming11/RobustDatasetDistillation",
        "url": "https://arxiv.org/abs/2402.04676",
        "cite": "vahidian2025group",
        "website": null
      }
    ],
    "fairness": [
      {
        "title": "Fair Graph Distillation",
        "author": "Qizhang Feng et al., NeurIPS 2023",
        "github": null,
        "url": "https://openreview.net/forum?id=xW0ayZxPWs",
        "cite": "feng2023fair",
        "website": null
      },
      {
        "title": "FairDD: Fair Dataset Distillation via Synchronized Matching",
        "author": "Qihang Zhou et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2411.19623",
        "cite": "zhou2024fair",
        "website": null
      }
    ],
    "text-application": [
      {
        "title": "Data Distillation for Text Classification",
        "author": "Yongqi Li et al., 2021",
        "github": null,
        "url": "https://arxiv.org/abs/2104.08448",
        "cite": "li2021text",
        "website": null
      },
      {
        "title": "Dataset Distillation with Attention Labels for Fine-tuning BERT",
        "author": "Aru Maekawa et al., ACL 2023",
        "github": "https://github.com/arumaekawa/dataset-distillation-with-attention-labels",
        "url": "https://aclanthology.org/2023.acl-short.12/",
        "cite": "maekawa2023text",
        "website": null
      },
      {
        "title": "DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation",
        "author": "Aru Maekawa et al., NAACL 2024",
        "github": "https://github.com/arumaekawa/DiLM",
        "url": "https://arxiv.org/abs/2404.00264",
        "cite": "maekawa2024dilm",
        "website": null
      },
      {
        "title": "Textual Dataset Distillation via Language Model Embedding",
        "author": "Yefan Tao et al., EMNLP 2024",
        "github": null,
        "url": "https://aclanthology.org/2024.findings-emnlp.733/",
        "cite": "tao2024textual",
        "website": null
      },
      {
        "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation",
        "author": "Huimin Lu et al., ICLR 2025",
        "github": "https://github.com/EminLU/UniDetox",
        "url": "https://arxiv.org/abs/2504.20500",
        "cite": "lu2025llm",
        "website": null
      },
      {
        "title": "Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training",
        "author": "Xunxin Cai & Chengrui Wang & Qingqing Long et al., DASFAA 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2501.15108",
        "cite": "cai2025llm",
        "website": null
      },
      {
        "title": "Synthetic Text Generation for Training Large Language Models via Gradient Matching",
        "author": "Dang Nguyen & Zeman Li et al., ICML 2025",
        "github": "https://github.com/BigML-CS-UCLA/GRADMM",
        "url": "https://arxiv.org/abs/2502.17607",
        "cite": "nguyen2025llm",
        "website": null
      }
    ],
    "video-application": [
      {
        "title": "Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement",
        "author": "Ziyu Wang & Yue Xu et al., CVPR 2024",
        "github": "https://github.com/yuz1wan/video_distillation",
        "url": "https://arxiv.org/abs/2312.00362",
        "cite": "wang2023dancing",
        "website": null
      },
      {
        "title": "Video Set Distillation: Information Diversification and Temporal Densifica",
        "author": "Yinjie Zhao et al., 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2412.00111",
        "cite": "zhao2024video",
        "website": null
      },
      {
        "title": "A Large-Scale Study on Video Action Dataset Condensation",
        "author": "Yang Chen et al., 2024",
        "github": "https://github.com/MCG-NJU/Video-DC",
        "url": "https://arxiv.org/abs/2412.21197",
        "cite": "chen2024video",
        "website": null
      },
      {
        "title": "Condensing Action Segmentation Datasets via Generative Network Inversion",
        "author": "Guodong Ding et al., CVPR 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2503.14112",
        "cite": "ding2025video",
        "website": null
      },
      {
        "title": "Latent Video Dataset Distillation",
        "author": "Ning Li et al., CVPR 2025 Workshop",
        "github": "https://github.com/liningresearch/Latent_Video_Dataset_Distillation",
        "url": "https://arxiv.org/abs/2504.17132",
        "cite": "li2025latent",
        "website": null
      }
    ],
    "tabular": [
      {
        "title": "New Properties of the Data Distillation Method When Working With Tabular Data",
        "author": "Dmitry Medvedev et al., AIST 2020",
        "github": "https://github.com/dm-medvedev/dataset-distillation",
        "url": "https://arxiv.org/abs/2010.09839",
        "cite": "medvedev2020tabular",
        "website": null
      }
    ],
    "retrieval": [
      {
        "title": "Towards Efficient Deep Hashing Retrieval: Condensing Your Data via Feature-Embedding Matching",
        "author": "Tao Feng & Jie Zhang et al., 2023",
        "github": null,
        "url": "https://arxiv.org/abs/2305.18076",
        "cite": "feng2023hash",
        "website": null
      }
    ],
    "domain-adaptation": [
      {
        "title": "Multi-Source Domain Adaptation Meets Dataset Distillation through Dataset Dictionary Learning",
        "author": "Montesuma et al., ICASSP 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2309.07666",
        "cite": "montesuma2024multi",
        "website": null
      }
    ],
    "super-resolution": [
      {
        "title": "GSDD: Generative Space Dataset Distillation for Image Super-resolution",
        "author": "Haiyu Zhang et al., AAAI 2024",
        "github": null,
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/28534",
        "cite": "zhang2024super",
        "website": null
      }
    ],
    "time-series": [
      {
        "title": "Dataset Condensation for Time Series Classification via Dual Domain Matching",
        "author": "Zhanyu Liu et al., KDD 2024",
        "github": "https://github.com/zhyliu00/TimeSeriesCond",
        "url": "https://arxiv.org/abs/2403.07245",
        "cite": "liu2024time",
        "website": null
      },
      {
        "title": "CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting",
        "author": "Jianrong Ding & Zhanyu Liu et al., NeurIPS 2024",
        "github": "https://github.com/RafaDD/CondTSF",
        "url": "https://arxiv.org/abs/2406.02131",
        "cite": "ding2024time",
        "website": null
      },
      {
        "title": "Less is More: Efficient Time Series Dataset Condensation via Two-fold Modal Matching",
        "author": "Hao Miao et al., VLDB 2025",
        "github": "https://github.com/uestc-liuzq/STdistillation",
        "url": "https://arxiv.org/abs/2410.20905",
        "cite": "miao2025timedc",
        "website": null
      }
    ],
    "speech": [
      {
        "title": "Dataset-Distillation Generative Model for Speech Emotion Recognition",
        "author": "Fabian Ritter-Gutierrez et al., Interspeech 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2406.02963",
        "cite": "fabian2024speech",
        "website": null
      }
    ],
    "machine-unlearning": [
      {
        "title": "Distilled Datamodel with Reverse Gradient Matching",
        "author": "Jingwen Ye et al., CVPR 2024",
        "github": null,
        "url": "https://arxiv.org/abs/2404.14006",
        "cite": "ye2024datamodel",
        "website": null
      },
      {
        "title": "Dataset Condensation Driven Machine Unlearning",
        "author": "Junaid Iqbal Khan, 2024",
        "github": "https://github.com/algebraicdianuj/DC_U",
        "url": "https://arxiv.org/abs/2402.00195",
        "cite": "khan2024unlearning",
        "website": null
      }
    ],
    "reinforcement-learning": [
      {
        "title": "Behaviour Distillation",
        "author": "Andrei Lupu et al., ICLR 2024",
        "github": "https://github.com/flairox/behaviour-distillation",
        "url": "https://arxiv.org/abs/2406.15042",
        "cite": "lupu2024bd",
        "website": null
      },
      {
        "title": "Dataset Distillation for Offline Reinforcement Learning",
        "author": "Jonathan Light & Yuanzhe Liu et al., ICML 2024 Workshop",
        "github": "https://github.com/ggflow123/DDRL",
        "url": "https://arxiv.org/abs/2407.20299",
        "cite": "light2024rl",
        "website": "https://datasetdistillation4rl.github.io/"
      },
      {
        "title": "Offline Behavior Distillation",
        "author": "Shiye Lei et al., NeurIPS 2024",
        "github": "https://github.com/LeavesLei/OBD",
        "url": "https://arxiv.org/abs/2410.22728",
        "cite": "lei2024obl",
        "website": null
      }
    ],
    "long": [
      {
        "title": "Distilling Long-tailed Datasets",
        "author": "Zhenghao Zhao & Haoxuan Wang et al., CVPR 2025",
        "github": "https://github.com/ichbill/LTDD",
        "url": "https://arxiv.org/abs/2408.14506",
        "cite": "zhao2025long",
        "website": null
      }
    ],
    "noisy": [
      {
        "title": "Dataset Distillers Are Good Label Denoisers In the Wild",
        "author": "Lechao Cheng et al., 2024",
        "github": "https://github.com/Kciiiman/DD_LNL",
        "url": "https://arxiv.org/abs/2411.11924",
        "cite": "cheng2024noisy",
        "website": null
      }
    ],
    "object": [
      {
        "title": "Fetch and Forge: Efficient Dataset Condensation for Object Detection",
        "author": "Ding Qi et al., NeurIPS 2024",
        "github": null,
        "url": "https://openreview.net/forum?id=m8MElyzuwp",
        "cite": "qi2024dcod",
        "website": null
      },
      {
        "title": "OD3: Optimization-free Dataset Distillation for Object Detection",
        "author": "Salwa K. Al Khatib & Ahmed ElHagry & Shitong Shao et al., 2025",
        "github": "https://github.com/VILA-Lab/OD3",
        "url": "https://arxiv.org/abs/2506.01942",
        "cite": "khatib2025od3",
        "website": null
      }
    ],
    "point": [
      {
        "title": "Point Cloud Dataset Distillation",
        "author": "Deyu Bo et al., ICML 2025",
        "github": null,
        "url": "https://openreview.net/forum?id=Us8v5tDOFd",
        "cite": "bo2025point",
        "website": null
      },
      {
        "title": "Dataset Distillation of 3D Point Clouds via Distribution Matching",
        "author": "Jae-Young Yim & Dongwook Kim et al., 2025",
        "github": null,
        "url": "https://arxiv.org/abs/2503.22154",
        "cite": "yim2025point",
        "website": null
      }
    ]
  }
}
